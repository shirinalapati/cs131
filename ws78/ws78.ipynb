import pandas as pd

# Sample data for the original file (could be anything, here I use simple integers)
sample_data = {
    'col1': range(1, 101),   # 100 rows of data
    'col2': range(101, 201)
}

df = pd.DataFrame(sample_data)

# Function to create different sizes of data files
def create_data_file(filename, rows_to_generate):
    df_large = pd.concat([df] * (rows_to_generate // len(df)), ignore_index=True)
    df_large.to_csv(filename, index=False)

# Generate data files with different sizes
create_data_file('data_200.csv', 200)
create_data_file('data_300.csv', 300)
create_data_file('data_1100.csv', 1100)
create_data_file('data_5500.csv', 5500)
create_data_file('data_10100.csv', 10100)

from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
import time

# Initialize Spark session
spark = SparkSession.builder.appName('DataProcessingApp').getOrCreate()

# Function to process a file
def process_file(file_path):
    # Start timing
    start_time = time.time()

    # Load the CSV data into a DataFrame
    df = spark.read.csv(file_path, header=True, inferSchema=True)

    # Perform a groupBy and average operation
    result = df.groupBy('col1').agg(avg('col2')).show()

    # End timing
    end_time = time.time()

    # Calculate and return the runtime
    runtime = end_time - start_time
    return runtime

# List of data files
data_files = ['data_200.csv', 'data_300.csv', 'data_1100.csv', 'data_5500.csv', 'data_10100.csv']

# Run the application on each file and record the runtime
runtimes = []
for file in data_files:
    runtime = process_file(file)
    print(f"Runtime for {file}: {runtime} seconds")
    runtimes.append((file, runtime))

# Stop the Spark session
spark.stop()

# Print out the runtime for each file
print(runtimes)

import matplotlib.pyplot as plt

# Data from the runtime measurements (you will fill this with the results from your PySpark runs)
file_sizes = [200, 300, 1100, 5500, 10100]  # Number of rows in each file
runtimes = [1.23, 1.45, 3.67, 15.12, 29.45]  # Replace with actual runtimes

# Plotting the graph
plt.plot(file_sizes, runtimes, marker='o', linestyle='-', color='b')
plt.title('Run Time vs File Size')
plt.xlabel('File Size (Number of Rows)')
plt.ylabel('Runtime (seconds)')
plt.grid(True)
plt.show()






